{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OxfordPet_CAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXXlyi8O1-2Q"
      },
      "source": [
        "# **CLASS ACTIVATION MAPS IN OXFORD PET III DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtOyGL9n-S_s"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook is my personal experimentation with Class Activation Maps (https://arxiv.org/abs/1512.04150), using the Oxford III Pet dataset (available here: https://www.kaggle.com/tanlikesmath/the-oxfordiiit-pet-dataset) for the evaluation. \n",
        "\n",
        "A reference implementation of this algorithm by Chae Young Lee can be found in this Git Hub repository: https://github.com/chaeyoung-lee/pytorch-CAM\n",
        "\n",
        "\n",
        "**Disclaimer: this notebook is yet to be finished. So far, the model, the get_cam function and the training loop have been defined. The ultimate milestone is to propose a weakly supervised segmentation algorithm that performs region growing on the Class Activation Maps*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uHEMP5Y-VUi"
      },
      "source": [
        "## Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giaxvh_2sDy"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbFBNj7E19jJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48vGCmYb2t3y"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uodvJIc32x8H"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.autograd import Function\n",
        "from torch.autograd import Variable\n",
        "import torch.cuda.amp as amp\n",
        "import json\n",
        "\n",
        "import random\n",
        "import scipy.io\n",
        "from PIL import Image\n",
        "import cv2 as cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6QsRyI73AeG"
      },
      "source": [
        "### Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Pv4LAgk3CDN"
      },
      "source": [
        "def one_hot(mask):\n",
        "  \"\"\"\n",
        "    This function one-hot encodes the segmentation Ground Truth masks. In Oxford III we have three classes:\n",
        "    '0': background, '1': boundary, '2': animal. \n",
        "  \"\"\"\n",
        "\n",
        "  new = np.zeros((3, mask.shape[0], mask.shape[1]))\n",
        "\n",
        "  new[0, (mask==1)] = 1 \n",
        "  new[1, (mask==2)] = 1\n",
        "  new[2, (mask==3)] = 1  \n",
        "  \n",
        "  new = new.transpose(1,2,0)\n",
        "  return new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCfdM3Dy6CNz"
      },
      "source": [
        "#Kaiming weight initialization\n",
        "def weights_init(m):\n",
        "    if isinstance(m, torch.nn.Conv2d):\n",
        "      torch.nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
        "      if m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0)\n",
        "    if isinstance(m, torch.nn.ConvTranspose2d):\n",
        "      torch.nn.init.kaiming_uniform_(m.weight.data,nonlinearity='relu')\n",
        "      if m.bias is not None:\n",
        "          nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoUJfozR2zsH"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X_42nzR21b3"
      },
      "source": [
        "#ImageNet mean/std. *To do: calculate mean/std of Oxford Pet III\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize([256,256]), \n",
        "     #transforms.RandomVerticalFlip(p=0.5), #Recortar Crop -> Data aug\n",
        "     #transforms.RandomHorizontalFlip(p=0.5)\n",
        "    ])\n",
        "\n",
        "data_aug = transforms.Compose(\n",
        "    [#transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.15), #High computational cost -> epoch training time x1.5\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=mean, std=std),\n",
        "     transforms.ToPILImage()\n",
        "     \n",
        "    ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mICvFMX33IiQ"
      },
      "source": [
        "### Oxford Pet III Database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMYo-fBG3LiG"
      },
      "source": [
        "class oxford_Pet(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, transform=None, data_aug=None, set='train'):\n",
        "    self.root = root\n",
        "\n",
        "    if(set=='train'): self.dataDir = root + 'paths/train.txt'\n",
        "    if(set=='val'): self.dataDir = root + 'paths/val.txt'\n",
        "    if(set=='test'): self.dataDir = root + 'paths/test.txt'\n",
        "\n",
        "    self.data = open(self.dataDir)\n",
        "    self.paths = self.data.readlines()\n",
        "    self.transform = transform\n",
        "    self.data_aug = data_aug\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    name, class_id, species, breed_id = self.paths[index].strip().split()\n",
        "    imgPath = root + 'images/' + name + '.jpg'\n",
        "    gtPath = root + 'annotations/trimaps/' + name + '.png'\n",
        "\n",
        "    img = Image.open(imgPath) #JPG file (3, h, w)\n",
        "    gt = Image.open(gtPath) #PNG file (1, h, w)\n",
        "    gt_t = one_hot(np.array(gt))\n",
        "    \n",
        "    if self.transform:\n",
        "      seed = np.random.randint(56346346)\n",
        "\n",
        "      random.seed(seed)\n",
        "      torch.manual_seed(seed)\n",
        "      img_t = self.transform(img)\n",
        "      \n",
        "      random.seed(seed)\n",
        "      torch.manual_seed(seed)\n",
        "      gt_t = self.transform(gt_t)\n",
        "\n",
        "    \n",
        "    species = int(species)\n",
        "    wl = np.zeros(2) \n",
        "    wl[species-1] = 1 #One hot\n",
        "    wl_t = torch.from_numpy(wl)  #To tensor\n",
        "    wl_t = wl_t.float() #To float\n",
        "\n",
        "    return img_t, gt_t, wl_t #Image, Segmentation mask, Classification mask\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.paths)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7QXkKXA3jA-"
      },
      "source": [
        "root='/content/drive/My Drive/Pet/'\n",
        "\n",
        "#Sets: 'train' (1846), 'val' (1834), 'test'\n",
        "\n",
        "trainset = oxford_Pet(root=root, transform=transform, set='train')\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "validationset = oxford_Pet(root=root, transform=transform, set='val_simplified')\n",
        "validationloader = torch.utils.data.DataLoader(validationset, batch_size=2, shuffle=True)\n",
        "\n",
        "testset = db_isic_Dataset(root=ROOT, idx=idx, transform=transform, set='test')\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=1)\n",
        "\n",
        "data_loaders = {\"train\": trainloader, \"val\": validationloader}\n",
        "\n",
        "print(\"val: \", len(validationset))\n",
        "print(\"train: \", len(trainset))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-gQsxWC3hsq"
      },
      "source": [
        "### Model arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Cj0wfG37HY"
      },
      "source": [
        "class net(torch.nn.Module):  \n",
        "  \"\"\"\n",
        "    This class creates a hybrid classification-segmentation model and returns three outputs given an input tensor:\n",
        "      - x: predicted segmentation masks.\n",
        "      - aspp: Atrous Spatial Pyramid Pooling feature maps (encoder).\n",
        "      - output: predicted classification vector.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.resnet = models.resnet101(pretrained=True)\n",
        "\n",
        "    #ENCODER (RESNET101 PRETRAINED)\n",
        "    self.layer1 = nn.Sequential(*list(self.resnet.children())[0:5])\n",
        "    self.layer2 = nn.Sequential(*list(self.resnet.layer2))\n",
        "\n",
        "    self.layer3 = nn.Sequential(\n",
        "      nn.Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=True),\n",
        "      nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=True),\n",
        "      nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(256, 512, kernel_size=1, stride=1, bias=True),\n",
        "      nn.BatchNorm2d(512),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Conv2d(1024, 1024, kernel_size=1, stride=1, bias=True),\n",
        "        nn.BatchNorm2d(1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(1024, 1024, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "        nn.BatchNorm2d(1024),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    #ASPP layers\n",
        "    self.a1 = nn.Sequential( #1x1 Conv Stride=2\n",
        "        nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1, stride=2, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.a2 = nn.Sequential( #Dilation 2\n",
        "        nn.Conv2d(in_channels= 1024, out_channels=256, kernel_size=3, stride=2, dilation=2, padding=2),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.a3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels= 1024, out_channels=256, kernel_size=3, stride=2, dilation=6, padding=6),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.a4 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels= 1024, out_channels=256, kernel_size=3, stride=2, dilation=8, padding=8),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.Conv2d(in_channels= 256, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU()   \n",
        "    )\n",
        "    self.a5 = nn.Sequential(\n",
        "        nn.MaxPool2d(kernel_size=2),\n",
        "        nn.Conv2d(in_channels= 1024, out_channels=256, kernel_size=1, stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    # Concatenate a_i [batch, 2048, hw]\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=1280, out_channels=1024, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),        \n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.upsample1 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=1536, out_channels=768, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 768, out_channels=768, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),       \n",
        "    )\n",
        "\n",
        "    self.upsample2 = nn.Sequential(\n",
        "        nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels= 512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.ConvTranspose2d(in_channels=256, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)         \n",
        "    )\n",
        "    \n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(in_features=1280, out_features=2, bias=True),\n",
        "        nn.Softmax()\n",
        "    )    \n",
        "\n",
        "     \n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    print_shape = False #For debugging purposes\n",
        "\n",
        "    #ENCODER--------------------------------------------------\n",
        "    if(print_shape): print(\"Input: \", input.shape)\n",
        "    l1 = self.layer1(input)\n",
        "    if(print_shape): print(\"(l1): \", l1.shape)\n",
        "    l2 = self.layer2(l1)\n",
        "    if(print_shape): print(\"(l2): \", l2.shape)\n",
        "    l3 = self.layer3(l2)\n",
        "    l3 = torch.cat((l3, l2), dim=1)\n",
        "    if(print_shape): print(\"(l3): \", l3.shape)\n",
        "    l3 = self.layer4(l3)\n",
        "    if(print_shape): print(\"(l4): \", l3.shape)\n",
        "\n",
        "    #ATROUS SPATIAL PYRAMID POOLING\n",
        "    a1 = self.a1(l3)\n",
        "    if(print_shape): print(\"(a1) Conv1x1: \", a1.shape)\n",
        "    a2 = self.a2(l3)\n",
        "    if(print_shape): print(\"(a2) Dilation=2: \", a2.shape)\n",
        "    a3 = self.a3(l3)\n",
        "    if(print_shape): print(\"(a3) Dilation=4: \", a3.shape)\n",
        "    a4 = self.a4(l3)\n",
        "    if(print_shape): print(\"(a4) Dilation=8: \", a4.shape)\n",
        "    a5 = self.a5(l3)\n",
        "    if(print_shape): print(\"(a5) Max pool: \", a5.shape)\n",
        "\n",
        "    aspp = torch.cat((a1, a2, a3, a4, a5), dim=1) #ASPP concatenation\n",
        "    if(print_shape): print(\"(aspp) Concat aspp: \", aspp.shape)\n",
        "    \n",
        "    #DECODER ------------------------------------------------------------\n",
        "    \n",
        "    x = self.conv1(aspp)\n",
        "    if(print_shape): print(\"\")\n",
        "    if(print_shape): print(\"(conv1) Conv1x1: \", x.shape)\n",
        "    \n",
        "\n",
        "    x = torch.cat((x, l2), dim=1) #Skip connection\n",
        "    if(print_shape): print(\"Concat skip conv1 + l2 \", x.shape)\n",
        "\n",
        "    x = self.upsample1(x)\n",
        "    if(print_shape): print(\"(upsample1) up1 \", x.shape)\n",
        "\n",
        "    x = torch.cat((x, l1), dim=1) #Skip connection\n",
        "    if(print_shape): print(\"Concat up1 + l1: \", x.shape)\n",
        "\n",
        "    x = self.upsample2(x)\n",
        "    if(print_shape): print(\"(upsample2) Output: \", x.shape)\n",
        "    \n",
        "    #CAM----------------------------------------------------------------\n",
        "\n",
        "    output = self.avgpool(aspp)\n",
        "    output = output.view(output.size(0), -1)\n",
        "\n",
        "    output = self.fc(output)\n",
        "\n",
        "    return x, aspp, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW98oo0K492w"
      },
      "source": [
        "### Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM5QrGuh49Sm"
      },
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "       \n",
        "    def forward(self, inputs, targets, smooth=1, alpha=1, gamma=2):\n",
        "            \n",
        "        #-----------------------------------------------------------------------\n",
        "        \n",
        "        #FOCAL CROSS ENTROPY----------------------------------------------------  \n",
        "        _, targets = torch.max(targets, dim=1) #[7, h, w] -> [1, h, w]\n",
        "            \n",
        "        BCE_loss = nn.CrossEntropyLoss(redution='none')(inputs, targets)\n",
        "\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = alpha * (1-pt)**gamma * BCE_loss\n",
        "        F_loss = torch.mean(F_loss)\n",
        "\n",
        "        return F_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQP0DFk6ldK"
      },
      "source": [
        "### Model and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBvReQ6m6nov"
      },
      "source": [
        "#Arquitecture: 'unet', 'unetV2, 'deeplab', 'deeplab_simplified'\n",
        "#Loss_fn: 'focal', 'cross entropy', 'cross entropy weighted', 'tversky', 'dice', 'focal tversky', 'focal v2'\n",
        "arquitecture = 'aspp'\n",
        "loss_fn = 'combo'\n",
        "optim_ = 'adam'\n",
        "\n",
        "#To GPU if available -------------------------------------------------------------\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "#Model-----------------------------------------------------------------------------\n",
        " \n",
        "\n",
        "if(arquitecture == 'aspp'):\n",
        "  model = net()\n",
        "  params_e3 = [p for p in model.layer3.parameters() if p.requires_grad] #From scratch\n",
        "  params_e4 = [p for p in model.layer4.parameters() if p.requires_grad]\n",
        "  \n",
        "  params_a1 = [p for p in model.a1.parameters() if p.requires_grad] \n",
        "  params_a2 = [p for p in model.a2.parameters() if p.requires_grad]\n",
        "  params_a3 = [p for p in model.a3.parameters() if p.requires_grad]\n",
        "  params_a4 = [p for p in model.a4.parameters() if p.requires_grad]\n",
        "  params_a5 = [p for p in model.a5.parameters() if p.requires_grad]\n",
        "  params_conv1 = [p for p in model.conv1.parameters() if p.requires_grad] #Decoder\n",
        "  params_up1 = [p for p in model.upsample1.parameters() if p.requires_grad]\n",
        "  params_up2 = [p for p in model.upsample2.parameters() if p.requires_grad]\n",
        "  params_fc = [p for p in model.fc.parameters() if p.requires_grad] #Fully Connected\n",
        "\n",
        "\n",
        "model.apply(weights_init)\n",
        "model.to(device)\n",
        "\n",
        "#Loss--------------------------------------------------------------------------------\n",
        "criterion = FocalLoss() #Segmentation\n",
        "\n",
        "weight_class = torch.tensor([0.66, 0.33])\n",
        "weight_class = weight_class.to(device)\n",
        "criterion2 = nn.BCELoss(weight=weight_class) #Classification\n",
        "\n",
        "#Optimizer ------------------------------------------------------------------------------------\n",
        "\n",
        "optimizer = optim.Adam([{'params': params_e3, 'lr': 5e-5}, #Encoder\n",
        "                        {'params': params_e4, 'lr': 5e-5},\n",
        "\n",
        "                        {'params': params_a1, 'lr': 5e-5}, #ASPP\n",
        "                        {'params': params_a2, 'lr': 5e-5},\n",
        "                        {'params': params_a3, 'lr': 5e-5},\n",
        "                        {'params': params_a4, 'lr': 5e-5},\n",
        "                        {'params': params_a5, 'lr': 5e-5},\n",
        "\n",
        "                        {'params': params_up1, 'lr': 5e-5}, #Decoder\n",
        "                        {'params': params_up2, 'lr': 5e-5},\n",
        "                        {'params': params_fc, 'lr': 1e-4} #Classifier\n",
        "                        ], lr=1e-3, weight_decay=1e-6)\n",
        "\n",
        "\n",
        "#Learning rate scheduler---------------------------------------------------------------------------\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Xv2WKl8gxk"
      },
      "source": [
        "## Get Class Activation Maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPmDpnZl8jL9"
      },
      "source": [
        "def get_cam(model, feature_map, num_classes=2):\n",
        "  \"\"\"\n",
        "    inputs:\n",
        "      · model: net() object with the parameters for all layers. Used to recover weights from GAP --> FC layer\n",
        "      · feature_map: last convolutional volume of encoder, with shape [batch, channels, h,w], where channels is \n",
        "        a design parameter. \n",
        "        - h,w ~ (16,16) or (8,8), output stride=16 (wrt input size)\n",
        "      · num_classes: number of classes. *Update needed to be class agnostic. Which variable here contains the num of classes?  \n",
        "    output: cam #[batch, num_classes, h, w]. Class activation maps for each class and batch\n",
        "  \"\"\"\n",
        "  params = list(model.parameters())\n",
        "  w = np.squeeze(params[-2].data.cpu().numpy()) #(6, 1536) e.g\n",
        "  batch, channels, height, width = feature_map.shape\n",
        " \n",
        "  cam = np.zeros((batch, num_classes, height, width))\n",
        "\n",
        "  for batch in range(batch):\n",
        "    f_b = feature_map[batch].detach().cpu().numpy() \n",
        "    f_b = f_b.reshape(channels, height*width) \n",
        "\n",
        "    cam_i = np.dot(w, f_b) #(2, 256)\n",
        "    cam_i = cam_i.reshape(num_classes, height, width) #(2, 16, 16)\n",
        "\n",
        "    cam[batch] = cam_i #(batch, 2, 16, 16)\n",
        "\n",
        "  return cam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyGu1wgt9Qer"
      },
      "source": [
        "def show_cam(cam):\n",
        "  \"\"\"\n",
        "    Visualize the first group of Class Activation Maps in a batch. Plots the class activation maps for the 2 classes: cat, dog.\n",
        "      input: cam #(batch, 6, h, w)\n",
        "  \"\"\"\n",
        "  batch, classes, h, w = cam.shape\n",
        "  cam = torch.from_numpy(cam) #to Tensor\n",
        "\n",
        "  upsample = nn.UpsamplingBilinear2d(scale_factor=16) #Rescale CAMs to the input/output resolution.\n",
        "  cam = upsample(cam)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(8,4))\n",
        "  plt.subplot(121)\n",
        "  plt.title(\"Cat\")\n",
        "  plt.imshow(cam[0, 0], cmap='gnuplot2')\n",
        "  plt.subplot(122)\n",
        "  plt.title(\"Dog\")\n",
        "  plt.imshow(cam[0, 1], cmap='gnuplot2')\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_C7as0L-P6E"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNKgyMXc-ROE"
      },
      "source": [
        "showImg = True \n",
        "checkpoint_path = \"/content/drive/My Drive/Checkpoints/model.tar\"\n",
        "\n",
        "saveCheckpoint = True #Saves the model / optimizer each epoch\n",
        "loadCheckpoint = False\n",
        "#---------------------------------------------------------------------------------\n",
        "#Loss\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "#Jaccard Score\n",
        "iou_train_score = []\n",
        "iou_val_score = []\n",
        "\n",
        "#Learning Rate\n",
        "lrate_backbone = []\n",
        "lrate_head = []\n",
        "\n",
        "running_loss = 0.0\n",
        "running_iou = 0.0\n",
        "\n",
        "min_loss = 100\n",
        "iou = 0\n",
        "j = 0 #Scheduler \n",
        "th = 120\n",
        "\n",
        "if(loadCheckpoint):\n",
        "  print(\"Loading checkpoint... | Path: \", checkpoint_path)\n",
        "  checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        " \n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    if(epoch > 0): #Guardar modelo al menos en la 1ª época\n",
        "      if(saveCheckpoint):\n",
        "          print(\"Saving checkpoint... | epoch: \", epoch, \" | model: \", arquitecture, \" | Path: \", checkpoint_path)\n",
        "          torch.save({'model_state_dict': model.state_dict(),  \n",
        "                      'optimizer_state_dict': optimizer.state_dict(),\n",
        "                      'scheduler_state': scheduler1.state_dict()},\n",
        "                      checkpoint_path)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()  # Set model to training mode\n",
        "            print(\"Epoch: \", epoch, \"| Training phase |\")\n",
        "        else:\n",
        "            model.eval()  # Set model to evaluate mode\n",
        "            print(\"Epoch: \", epoch, \"| Validation phase |\")\n",
        "\n",
        "        for i, data in enumerate(trainloader,0):\n",
        "          inputs, labels, wl = data\n",
        "          labels = labels.long()          \n",
        "\n",
        "          inputs, labels, wl = inputs.to(device), labels.to(device), wl.to(device)\n",
        "          outputs, feature_map, out_c = model(inputs) #Output mask (b, 7,h,w) \n",
        "                                                      #Feature map (b, 1536, h/16, w/16)\n",
        "                                                      #Output vector (b, 2)\n",
        "\n",
        "          y_pred = outputs\n",
        "          \n",
        "          loss = criterion(y_pred, labels)\n",
        "          #loss = criterion2(out_c, wl)\n",
        " \n",
        "          _, y_pred = torch.max(y_pred,dim=1) #Undo one-hot        \n",
        "          __, labels = torch.max(labels, dim=1) #Undo one-hot\n",
        "\n",
        "          y_pred = y_pred.data.cpu().numpy() #To numpy\n",
        "          y_true = labels.data.cpu().numpy()\n",
        "\n",
        "          iou = jaccard_score(y_pred, y_true)\n",
        "          \n",
        "          if(i%10==0):\n",
        "            if(showImg):\n",
        "              print(\"\")\n",
        "              #print(\"Phase: \", phase)\n",
        "              plt.figure(figsize=(15,6))\n",
        "\n",
        "              plt.subplot(131)\n",
        "              plt.title('Input')\n",
        "              plt.imshow(inputs.cpu().data[0].squeeze().permute(1,2,0))\n",
        "              \n",
        "              plt.subplot(132)\n",
        "              plt.title('Prediction')\n",
        "              plt.imshow(y_pred[0])\n",
        "              print(\"Prediction values: \", np.unique(y_pred[0]))\n",
        "                            \n",
        "              plt.subplot(133)\n",
        "              plt.title(\"Label\")            \n",
        "              print(\"Label: \", np.unique(y_true[0]))\n",
        "              plt.imshow(y_true[0])\n",
        "                    \n",
        "              plt.show()\n",
        "\n",
        "              cam = get_cam(model=model, feature_map=feature_map, num_classes=2) #Get Class Activation Maps\n",
        "              show_cam(cam) #Plot the CAMs\n",
        "\n",
        "              out_c0 = out_c[0].detach().cpu().numpy()\n",
        "              print(\"Prediction: \", out_c0) #Classification prediction\n",
        "              print(\"Label: \", wl[0].cpu().numpy()) #Classification label\n",
        "\n",
        "         \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()     \n",
        "          scheduler.step()\n",
        "          \n",
        "          if(i%2 == 0):\n",
        "            train_loss.append(loss.item())\n",
        "            iou_train_score.append(iou)\n",
        "\n",
        "\n",
        "          if(i%25 ==0):\n",
        "            print(\"Loss: %.4f\" % loss.item(), \" | Jaccard Score: %.4f\" % iou)\n",
        "            \n",
        "            plt.figure(figsize=(12,6))\n",
        "            plt.subplot(121)\n",
        "            plt.title(\"Training Loss\")\n",
        "            plt.plot(train_loss, 'r')\n",
        "            plt.subplot(122)\n",
        "            plt.title(\"Training IoU\")\n",
        "            plt.plot(iou_train_score, 'y')\n",
        "            plt.show()          "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}